#!/usr/bin/env python3
"""
å°çº¢ä¹¦æœç´¢æ ¸å¿ƒè„šæœ¬ (ç”Ÿäº§çº§)
é›†æˆæ‰€æœ‰æµ‹è¯•åŠŸèƒ½,æ”¯æŒæœç´¢ã€ç¿»é¡µã€æ•°æ®ç­›é€‰å’Œä¿å­˜
"""
import http.client
import json
import os
from datetime import datetime
from urllib.parse import urlencode
from typing import Dict, List, Optional, Any
import re
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


class XiaohongshuSearcher:
    """å°çº¢ä¹¦æœç´¢å®¢æˆ·ç«¯"""

    def __init__(
        self,
        token: Optional[str] = None,
        host: str = "api.tikhub.io",
        path: str = "/api/v1/xiaohongshu/app/search_notes",
        timeout: int = 30
    ):
        """
        åˆå§‹åŒ–æœç´¢å®¢æˆ·ç«¯

        Args:
            token: API Token (é»˜è®¤ä»ç¯å¢ƒå˜é‡TIKHUB_TOKENè¯»å–)
            host: APIä¸»æœºåœ°å€
            path: APIè·¯å¾„
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´(ç§’)
        """
        self.token = token or os.getenv("TIKHUB_TOKEN", "")
        self.host = host
        self.path = path
        self.timeout = timeout
        self.search_id = ""
        self.session_id = ""

    def search(
        self,
        keyword: str,
        page: int = 1,
        sort_type: str = "general",
        filter_note_type: str = "ä¸é™",
        filter_note_time: str = "ä¸é™",
        reset_session: bool = False
    ) -> Dict[str, Any]:
        """
        æœç´¢å°çº¢ä¹¦ç¬”è®°

        Args:
            keyword: æœç´¢å…³é”®è¯
            page: é¡µç ,ä»1å¼€å§‹
            sort_type: æ’åºæ–¹å¼ (general/time_descending/popularity_descending/comment_descending/collect_descending)
            filter_note_type: ç¬”è®°ç±»å‹ç­›é€‰ (ä¸é™/è§†é¢‘ç¬”è®°/æ™®é€šç¬”è®°)
            filter_note_time: æ—¶é—´ç­›é€‰ (ä¸é™/ä¸€å¤©å†…/ä¸€å‘¨å†…/åŠå¹´å†…)
            reset_session: æ˜¯å¦é‡ç½®ä¼šè¯(æ–°å…³é”®è¯æ—¶ä½¿ç”¨)

        Returns:
            dict: APIå“åº”æ•°æ®
        """
        # é‡ç½®ä¼šè¯
        if reset_session:
            self.search_id = ""
            self.session_id = ""

        # æ„å»ºå‚æ•°
        params = {
            "keyword": keyword,
            "page": page,
            "search_id": self.search_id,
            "session_id": self.session_id,
            "sort_type": sort_type,
            "filter_note_type": filter_note_type,
            "filter_note_time": filter_note_time,
        }

        # å‘é€è¯·æ±‚
        query = urlencode(params, doseq=True)
        full_path = f"{self.path}?{query}" if query else self.path

        conn = http.client.HTTPSConnection(self.host, timeout=self.timeout)
        headers = {
            "Authorization": f"Bearer {self.token}",
            "Accept": "application/json",
        }

        conn.request("GET", full_path, headers=headers)
        res = conn.getresponse()
        raw = res.read()
        conn.close()

        # è§£æå“åº”
        text = raw.decode("utf-8", errors="replace")
        try:
            data = json.loads(text)
        except json.JSONDecodeError:
            return {
                "success": False,
                "error": "JSONè§£æå¤±è´¥",
                "raw": text,
                "_http_status": res.status
            }

        # ä¿å­˜ç¿»é¡µå‚æ•°
        if data.get("code") == 200:
            inner_data = data.get("data", {})
            if isinstance(inner_data, str):
                try:
                    inner_data = json.loads(inner_data)
                except:
                    pass

            if isinstance(inner_data, dict):
                self.search_id = inner_data.get("searchId", "")
                self.session_id = inner_data.get("sessionId", "")

        data["_http_status"] = res.status
        return data

    def extract_items(self, result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        ä»APIå“åº”ä¸­æå–ç¬”è®°åˆ—è¡¨

        Args:
            result: APIå“åº”æ•°æ®

        Returns:
            list: ç¬”è®°åˆ—è¡¨
        """
        items = []
        data = result.get("data") if isinstance(result, dict) else None

        if isinstance(data, dict):
            inner = data.get("data")
            if isinstance(inner, dict):
                items = inner.get("items", [])
            else:
                items = data.get("items", [])
        elif isinstance(data, str):
            try:
                inner_data = json.loads(data)
                if isinstance(inner_data, dict):
                    items = inner_data.get("data", {}).get("items", [])
            except:
                pass

        return items

    def extract_core_info(self, result: Dict[str, Any], keyword: str = "") -> Dict[str, Any]:
        """
        æå–æ ¸å¿ƒä¿¡æ¯

        Args:
            result: APIå“åº”æ•°æ®
            keyword: æœç´¢å…³é”®è¯

        Returns:
            dict: åŒ…å«æ ¸å¿ƒä¿¡æ¯çš„å­—å…¸
        """
        # æå–æœç´¢ä¿¡æ¯
        search_info = {
            "keyword": keyword,
            "search_time": datetime.now().isoformat(),
            "search_id": self.search_id,
            "session_id": self.session_id,
        }

        # æå–ç¬”è®°åˆ—è¡¨
        items = self.extract_items(result)
        notes = []

        for entry in items:
            if not isinstance(entry, dict):
                continue

            note = entry.get("note")
            if not isinstance(note, dict):
                continue

            # æå–åŸºæœ¬ä¿¡æ¯
            title = (note.get("title") or "").strip()
            desc = (note.get("desc") or "").strip()

            # æå–æ ‡ç­¾
            tags = self._extract_tags(title, desc)

            # æå–ä½œè€…ä¿¡æ¯
            user = note.get("user") or {}
            author = {
                "user_id": user.get("userid"),
                "red_id": user.get("red_id"),
                "nickname": user.get("nickname"),
                "avatar": user.get("images"),
            }

            # æå–äº’åŠ¨æ•°æ®
            interact_info = note.get("interact_info") or {}
            stats = {
                "liked_count": note.get("liked_count") or interact_info.get("liked_count", 0),
                "collected_count": note.get("collected_count") or interact_info.get("collected_count", 0),
                "comments_count": note.get("comments_count") or interact_info.get("comments_count", 0),
                "shared_count": note.get("shared_count") or interact_info.get("shared_count", 0),
            }

            # æå–åª’ä½“ä¿¡æ¯
            media = self._extract_media_info(note)

            # ç¬”è®°ç±»å‹
            note_type = note.get("type", "unknown")
            if note_type == "normal":
                note_type_cn = "å›¾æ–‡ç¬”è®°"
            elif note_type == "video":
                note_type_cn = "è§†é¢‘ç¬”è®°"
            else:
                note_type_cn = note_type

            note_info = {
                "note_id": note.get("id") or note.get("note_id"),
                "title": title,
                "desc": desc[:200] + "..." if len(desc) > 200 else desc,  # é™åˆ¶æè¿°é•¿åº¦
                "tags": tags,
                "note_type": note_type,
                "note_type_cn": note_type_cn,
                "author": author,
                "stats": stats,
                "media": media,
                "publish_time": note.get("timestamp"),
                "update_time": note.get("update_time"),
            }

            notes.append(note_info)

        return {
            "search_info": search_info,
            "total_count": len(notes),
            "notes": notes,
        }

    def _extract_tags(self, title: str, desc: str) -> List[str]:
        """æå–æ ‡ç­¾"""
        tags = []

        # ä»æ ‡é¢˜å’Œæè¿°ä¸­æå– #æ ‡ç­¾
        for text in [title, desc]:
            if not text:
                continue
            matches = re.findall(r"#([^#\\s]+)", text)
            tags.extend([f"#{match}" for match in matches])

        # å»é‡
        seen = set()
        unique_tags = []
        for tag in tags:
            if tag not in seen:
                seen.add(tag)
                unique_tags.append(tag)

        return unique_tags

    def _extract_media_info(self, note: Dict[str, Any]) -> Dict[str, Any]:
        """æå–åª’ä½“ä¿¡æ¯"""
        media = {
            "type": note.get("type", "unknown"),
            "images": [],
            "videos": [],
        }

        # æå–å›¾ç‰‡
        images_list = note.get("images_list") or []
        for img in images_list[:9]:  # æœ€å¤š9å¼ å›¾
            if isinstance(img, dict):
                image_info = {
                    "url": img.get("url"),
                    "url_large": img.get("url_size_large"),
                    "width": img.get("width"),
                    "height": img.get("height"),
                }
                media["images"].append(image_info)

        # æå–è§†é¢‘
        video_info = note.get("video_info_v2")
        if video_info:
            media_data = video_info.get("media", {})
            streams = media_data.get("stream", {})

            # H264
            h264_list = streams.get("h264", [])
            if h264_list:
                video_url = h264_list[0].get("master_url")
                media["videos"].append({
                    "type": "h264",
                    "url": video_url,
                    "quality": "HD",
                })

            # H265
            h265_list = streams.get("h265", [])
            for stream in h265_list[:2]:  # æœ€å¤š2ä¸ªè´¨é‡
                quality_type = stream.get("quality_type", "unknown")
                media["videos"].append({
                    "type": "h265",
                    "url": stream.get("master_url"),
                    "quality": quality_type,
                })

        return media


def save_results(
    result: Dict[str, Any],
    core_info: Dict[str, Any],
    save_dir: str = "responses"
) -> tuple[str, str]:
    """
    ä¿å­˜ç»“æœåˆ°æ–‡ä»¶

    Args:
        result: å®Œæ•´çš„APIå“åº”
        core_info: æå–çš„æ ¸å¿ƒä¿¡æ¯
        save_dir: ä¿å­˜ç›®å½•

    Returns:
        tuple: (å®Œæ•´å“åº”æ–‡ä»¶è·¯å¾„, æ ¸å¿ƒä¿¡æ¯æ–‡ä»¶è·¯å¾„)
    """
    os.makedirs(save_dir, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    keyword = core_info.get("search_info", {}).get("keyword", "unknown")

    # 1. ä¿å­˜å®Œæ•´å“åº”
    full_filename = f"{timestamp}_xhs_search_{keyword}_full.json"
    full_path = os.path.join(save_dir, full_filename)
    with open(full_path, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    # 2. ä¿å­˜æ ¸å¿ƒä¿¡æ¯
    core_filename = f"{timestamp}_xhs_search_{keyword}_core.json"
    core_path = os.path.join(save_dir, core_filename)
    with open(core_path, "w", encoding="utf-8") as f:
        json.dump(core_info, f, ensure_ascii=False, indent=2)

    return full_path, core_path


def main():
    """ä¸»å‡½æ•° - ç¤ºä¾‹ç”¨æ³•"""
    # åˆå§‹åŒ–æœç´¢å®¢æˆ·ç«¯
    searcher = XiaohongshuSearcher()

    # æœç´¢
    keyword = "çŒ«ç²®"
    print(f"ğŸ” æœç´¢å…³é”®è¯: {keyword}")

    result = searcher.search(
        keyword=keyword,
        page=1,
        sort_type="general",
        filter_note_type="ä¸é™",
        filter_note_time="ä¸é™",
        reset_session=True
    )

    # æ£€æŸ¥å“åº”
    if result.get("code") != 200:
        print(f"âŒ æœç´¢å¤±è´¥: {result.get('message')}")
        return

    print(f"âœ… æœç´¢æˆåŠŸ,çŠ¶æ€ç : {result.get('code')}")

    # æå–æ ¸å¿ƒä¿¡æ¯
    core_info = searcher.extract_core_info(result, keyword)

    print(f"ğŸ“Š æœç´¢ç»“æœ:")
    print(f"   - è¿”å›æ•°é‡: {core_info.get('total_count')}")
    print(f"   - Search ID: {core_info.get('search_info', {}).get('search_id')}")
    print(f"   - Session ID: {core_info.get('search_info', {}).get('session_id')}")

    # æ˜¾ç¤ºå‰3æ¡
    notes = core_info.get("notes", [])
    if notes:
        print(f"\nğŸ“ å‰3æ¡ç¬”è®°:")
        for i, note in enumerate(notes[:3], 1):
            print(f"\n   {i}. {note.get('title')}")
            print(f"      ç±»å‹: {note.get('note_type_cn')}")
            print(f"      ä½œè€…: {note.get('author', {}).get('nickname')}")
            print(f"      ç‚¹èµ: {note.get('stats', {}).get('liked_count')}")
            print(f"      æ”¶è—: {note.get('stats', {}).get('collected_count')}")
            print(f"      è¯„è®º: {note.get('stats', {}).get('comments_count')}")
            tags = note.get('tags', [])
            if tags:
                print(f"      æ ‡ç­¾: {', '.join(tags[:5])}")

    # ä¿å­˜ç»“æœ
    full_path, core_path = save_results(result, core_info)
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜:")
    print(f"   - å®Œæ•´å“åº”: {full_path}")
    print(f"   - æ ¸å¿ƒä¿¡æ¯: {core_path}")


if __name__ == "__main__":
    main()
