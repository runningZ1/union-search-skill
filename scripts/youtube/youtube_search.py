#!/usr/bin/env python3
"""
YouTube è§†é¢‘æœç´¢å·¥å…· (åŸºäº YouTube Data API v3)
æ”¯æŒæœç´¢è§†é¢‘ã€è·å–è¯¦ç»†ä¿¡æ¯ã€äº’åŠ¨æ•°æ®ã€è¯„è®ºåŒºç­‰
"""

import argparse
import json
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
from urllib.parse import urlencode
from urllib.request import Request, urlopen
from urllib.error import HTTPError, URLError
import re


def load_env_file(path: str):
    """åŠ è½½ç¯å¢ƒå˜é‡æ–‡ä»¶"""
    if not path or not os.path.exists(path):
        return
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith("#") or "=" not in line:
                continue
            key, value = line.split("=", 1)
            key = key.strip()
            value = value.strip()
            if key and key not in os.environ:
                os.environ[key] = value


def parse_duration(duration: str) -> str:
    """
    è§£æ ISO 8601 æ—¶é•¿æ ¼å¼ (PT1H2M10S) ä¸ºå¯è¯»æ ¼å¼

    Args:
        duration: ISO 8601 æ ¼å¼çš„æ—¶é•¿å­—ç¬¦ä¸²

    Returns:
        æ ¼å¼åŒ–çš„æ—¶é•¿å­—ç¬¦ä¸² (ä¾‹: "1:02:10")
    """
    match = re.match(r'PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?', duration)
    if not match:
        return duration

    hours = int(match.group(1) or 0)
    minutes = int(match.group(2) or 0)
    seconds = int(match.group(3) or 0)

    if hours > 0:
        return f"{hours}:{minutes:02d}:{seconds:02d}"
    else:
        return f"{minutes}:{seconds:02d}"


def make_api_request(url: str, params: Dict) -> Dict:
    """
    å‘é€ API è¯·æ±‚

    Args:
        url: API ç«¯ç‚¹ URL
        params: è¯·æ±‚å‚æ•°

    Returns:
        API å“åº”çš„ JSON æ•°æ®
    """
    query_string = urlencode(params)
    full_url = f"{url}?{query_string}"

    try:
        req = Request(full_url)
        req.add_header('User-Agent', 'Mozilla/5.0')

        with urlopen(req, timeout=30) as response:
            data = response.read()
            return json.loads(data.decode('utf-8'))
    except HTTPError as e:
        error_body = e.read().decode('utf-8')
        try:
            error_data = json.loads(error_body)
            error_msg = error_data.get('error', {}).get('message', str(e))
        except:
            error_msg = str(e)
        raise Exception(f"API è¯·æ±‚å¤±è´¥: {error_msg}")
    except URLError as e:
        raise Exception(f"ç½‘ç»œé”™è¯¯: {e.reason}")
    except Exception as e:
        raise Exception(f"è¯·æ±‚å¤±è´¥: {str(e)}")


def search_videos(
    api_key: str,
    keyword: str,
    limit: int = 10,
    order: str = "relevance",
    region_code: str = "US",
    language: str = "zh-CN",
) -> List[str]:
    """
    æœç´¢ YouTube è§†é¢‘

    Args:
        api_key: YouTube Data API å¯†é’¥
        keyword: æœç´¢å…³é”®è¯
        limit: è¿”å›ç»“æœæ•°é‡
        order: æ’åºæ–¹å¼ (relevance, date, rating, viewCount, title)
        region_code: åœ°åŒºä»£ç 
        language: è¯­è¨€ä»£ç 

    Returns:
        è§†é¢‘ ID åˆ—è¡¨
    """
    url = "https://www.googleapis.com/youtube/v3/search"
    params = {
        "key": api_key,
        "q": keyword,
        "part": "snippet",
        "type": "video",
        "maxResults": min(limit, 50),  # API é™åˆ¶æœ€å¤š 50
        "order": order,
        "regionCode": region_code,
        "relevanceLanguage": language,
    }

    result = make_api_request(url, params)

    video_ids = []
    for item in result.get("items", []):
        video_id = item.get("id", {}).get("videoId")
        if video_id:
            video_ids.append(video_id)

    return video_ids


def get_video_details(
    api_key: str,
    video_ids: List[str],
    include_comments: bool = False,
    max_comments: int = 10,
) -> List[Dict]:
    """
    è·å–è§†é¢‘è¯¦ç»†ä¿¡æ¯

    Args:
        api_key: YouTube Data API å¯†é’¥
        video_ids: è§†é¢‘ ID åˆ—è¡¨
        include_comments: æ˜¯å¦åŒ…å«è¯„è®º
        max_comments: æ¯ä¸ªè§†é¢‘çš„æœ€å¤§è¯„è®ºæ•°

    Returns:
        è§†é¢‘è¯¦ç»†ä¿¡æ¯åˆ—è¡¨
    """
    if not video_ids:
        return []

    # è·å–è§†é¢‘è¯¦ç»†ä¿¡æ¯
    url = "https://www.googleapis.com/youtube/v3/videos"
    params = {
        "key": api_key,
        "id": ",".join(video_ids),
        "part": "snippet,statistics,contentDetails",
    }

    result = make_api_request(url, params)

    videos = []
    for idx, item in enumerate(result.get("items", []), 1):
        snippet = item.get("snippet", {})
        statistics = item.get("statistics", {})
        content_details = item.get("contentDetails", {})

        video_data = {
            "rank": idx,
            "video_id": item.get("id", ""),
            "title": snippet.get("title", ""),
            "channel_title": snippet.get("channelTitle", ""),
            "channel_id": snippet.get("channelId", ""),
            "published_at": snippet.get("publishedAt", ""),
            "description": snippet.get("description", ""),
            "thumbnails": snippet.get("thumbnails", {}),
            "tags": snippet.get("tags", []),
            "category_id": snippet.get("categoryId", ""),
            "duration": parse_duration(content_details.get("duration", "")),
            "duration_raw": content_details.get("duration", ""),
            "definition": content_details.get("definition", ""),
            "caption": content_details.get("caption", ""),
            "statistics": {
                "view_count": int(statistics.get("viewCount", 0)),
                "like_count": int(statistics.get("likeCount", 0)),
                "comment_count": int(statistics.get("commentCount", 0)),
            },
            "url": f"https://www.youtube.com/watch?v={item.get('id', '')}",
        }

        # è·å–è¯„è®º
        if include_comments:
            try:
                comments = get_video_comments(api_key, item.get("id", ""), max_comments)
                video_data["comments"] = comments
            except Exception as e:
                video_data["comments_error"] = str(e)

        videos.append(video_data)

    return videos


def get_video_comments(
    api_key: str,
    video_id: str,
    max_results: int = 10,
) -> List[Dict]:
    """
    è·å–è§†é¢‘è¯„è®º

    Args:
        api_key: YouTube Data API å¯†é’¥
        video_id: è§†é¢‘ ID
        max_results: æœ€å¤§è¯„è®ºæ•°

    Returns:
        è¯„è®ºåˆ—è¡¨
    """
    url = "https://www.googleapis.com/youtube/v3/commentThreads"
    params = {
        "key": api_key,
        "videoId": video_id,
        "part": "snippet",
        "maxResults": min(max_results, 100),
        "order": "relevance",  # æŒ‰ç›¸å…³æ€§æ’åºï¼ˆçƒ­é—¨è¯„è®ºï¼‰
    }

    try:
        result = make_api_request(url, params)
    except Exception as e:
        # è¯„è®ºå¯èƒ½è¢«ç¦ç”¨
        if "disabled comments" in str(e).lower():
            return []
        raise

    comments = []
    for item in result.get("items", []):
        top_comment = item.get("snippet", {}).get("topLevelComment", {})
        snippet = top_comment.get("snippet", {})

        comment_data = {
            "author": snippet.get("authorDisplayName", ""),
            "author_channel_id": snippet.get("authorChannelId", {}).get("value", ""),
            "text": snippet.get("textDisplay", ""),
            "like_count": snippet.get("likeCount", 0),
            "published_at": snippet.get("publishedAt", ""),
            "updated_at": snippet.get("updatedAt", ""),
        }
        comments.append(comment_data)

    return comments


def format_text_output(results: List[Dict], keyword: str, include_comments: bool):
    """æ ¼å¼åŒ–æ–‡æœ¬è¾“å‡º"""
    print(f"\n{'='*80}")
    print(f"ğŸ” æœç´¢å…³é”®è¯: {keyword}")
    print(f"ğŸ“Š ç»“æœæ•°é‡: {len(results)}")
    print(f"{'='*80}\n")

    for result in results:
        print(f"{'='*80}")
        print(f"ğŸ“¹ è§†é¢‘ #{result['rank']}")
        print(f"{'='*80}")
        print(f"\nã€åŸºç¡€ä¿¡æ¯ã€‘")
        print(f"æ ‡é¢˜: {result['title']}")
        print(f"è§†é¢‘ID: {result['video_id']}")
        print(f"é¢‘é“: {result['channel_title']}")
        print(f"é¢‘é“ID: {result['channel_id']}")
        print(f"å‘å¸ƒæ—¶é—´: {result['published_at']}")
        print(f"æ—¶é•¿: {result['duration']}")
        print(f"è§†é¢‘é“¾æ¥: {result['url']}")

        print(f"\nã€äº’åŠ¨æ•°æ®ã€‘")
        stats = result['statistics']
        print(f"â–¶ï¸  æ’­æ”¾é‡: {stats['view_count']:,}")
        print(f"ğŸ’– ç‚¹èµæ•°: {stats['like_count']:,}")
        print(f"ğŸ’­ è¯„è®ºæ•°: {stats['comment_count']:,}")

        print(f"\nã€è§†é¢‘ä¿¡æ¯ã€‘")
        print(f"åˆ†ç±»ID: {result['category_id']}")
        print(f"æ¸…æ™°åº¦: {result['definition'].upper()}")
        print(f"å­—å¹•: {'æœ‰' if result['caption'] == 'true' else 'æ— '}")

        if result.get('tags'):
            print(f"\nã€è§†é¢‘æ ‡ç­¾ã€‘")
            print(f"æ ‡ç­¾: {', '.join(result['tags'][:10])}")

        if result.get('description'):
            print(f"\nã€è§†é¢‘ç®€ä»‹ã€‘")
            desc = result['description'][:200] + '...' if len(result['description']) > 200 else result['description']
            print(f"{desc}")

        if include_comments and 'comments' in result:
            print(f"\nã€çƒ­é—¨è¯„è®ºã€‘")
            for i, comment in enumerate(result['comments'][:5], 1):
                print(f"\nè¯„è®º #{i}")
                print(f"ä½œè€…: {comment['author']}")
                print(f"ç‚¹èµ: {comment['like_count']}")
                print(f"å†…å®¹: {comment['text'][:150]}")

        if 'comments_error' in result:
            print(f"\nâš ï¸  è¯„è®ºè·å–å¤±è´¥: {result['comments_error']}")

        print()


def format_markdown_output(results: List[Dict], keyword: str, include_comments: bool) -> str:
    """æ ¼å¼åŒ– Markdown è¾“å‡º"""
    md = f"# YouTube è§†é¢‘æœç´¢ç»“æœ\n\n"
    md += f"**æœç´¢å…³é”®è¯**: {keyword}\n\n"
    md += f"**ç»“æœæ•°é‡**: {len(results)}\n\n"
    md += f"---\n\n"

    for result in results:
        md += f"## è§†é¢‘ #{result['rank']}: {result['title']}\n\n"

        # ç¼©ç•¥å›¾
        thumbnails = result.get('thumbnails', {})
        if 'high' in thumbnails:
            md += f"![{result['title']}]({thumbnails['high']['url']})\n\n"

        md += f"### åŸºç¡€ä¿¡æ¯\n\n"
        md += f"| é¡¹ç›® | å†…å®¹ |\n"
        md += f"|------|------|\n"
        md += f"| **æ ‡é¢˜** | {result['title']} |\n"
        md += f"| **è§†é¢‘ID** | {result['video_id']} |\n"
        md += f"| **é¢‘é“** | {result['channel_title']} |\n"
        md += f"| **é¢‘é“ID** | {result['channel_id']} |\n"
        md += f"| **å‘å¸ƒæ—¶é—´** | {result['published_at']} |\n"
        md += f"| **æ—¶é•¿** | {result['duration']} |\n"
        md += f"| **è§†é¢‘é“¾æ¥** | [ç‚¹å‡»è§‚çœ‹]({result['url']}) |\n\n"

        stats = result['statistics']
        md += f"### äº’åŠ¨æ•°æ®\n\n"
        md += f"| æŒ‡æ ‡ | æ•°å€¼ |\n"
        md += f"|------|------|\n"
        md += f"| â–¶ï¸ **æ’­æ”¾é‡** | {stats['view_count']:,} |\n"
        md += f"| ğŸ’– **ç‚¹èµæ•°** | {stats['like_count']:,} |\n"
        md += f"| ğŸ’­ **è¯„è®ºæ•°** | {stats['comment_count']:,} |\n\n"

        if result.get('description'):
            md += f"### è§†é¢‘ç®€ä»‹\n\n{result['description']}\n\n"

        if result.get('tags'):
            md += f"### æ ‡ç­¾\n\n{', '.join(result['tags'])}\n\n"

        if include_comments and 'comments' in result:
            md += f"### çƒ­é—¨è¯„è®º\n\n"
            for i, comment in enumerate(result['comments'], 1):
                md += f"**{i}. {comment['author']}** (ğŸ‘ {comment['like_count']})\n\n"
                md += f"{comment['text']}\n\n"

        md += f"---\n\n"

    return md


def parse_args():
    parser = argparse.ArgumentParser(
        description="YouTube è§†é¢‘æœç´¢å·¥å…· (åŸºäº YouTube Data API v3)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python youtube_search.py "Python tutorial" --limit 5
  python youtube_search.py "æœºå™¨å­¦ä¹ " --order viewCount --limit 10
  python youtube_search.py "AI" --json --pretty
  python youtube_search.py "ç¼–ç¨‹" --markdown -o results.md
  python youtube_search.py "æ•™ç¨‹" --include-comments --max-comments 5

æ’åºæ–¹å¼:
  relevance  - ç›¸å…³æ€§ (é»˜è®¤)
  date       - å‘å¸ƒæ—¥æœŸ
  rating     - è¯„åˆ†
  viewCount  - æ’­æ”¾é‡
  title      - æ ‡é¢˜
"""
    )

    parser.add_argument("keyword", nargs="?", help="æœç´¢å…³é”®è¯")
    parser.add_argument("--keyword", dest="keyword_opt", help="æœç´¢å…³é”®è¯ (è¦†ç›–ä½ç½®å‚æ•°)")
    parser.add_argument("--api-key", help="YouTube Data API å¯†é’¥")
    parser.add_argument("--limit", type=int, default=10, help="è¿”å›ç»“æœæ•°é‡ (é»˜è®¤: 10, æœ€å¤§: 50)")
    parser.add_argument("--order", choices=["relevance", "date", "rating", "viewCount", "title"],
                       default="relevance", help="æ’åºæ–¹å¼ (é»˜è®¤: relevance)")
    parser.add_argument("--region", default="US", help="åœ°åŒºä»£ç  (é»˜è®¤: US)")
    parser.add_argument("--language", default="zh-CN", help="è¯­è¨€ä»£ç  (é»˜è®¤: zh-CN)")
    parser.add_argument("--include-comments", action="store_true", help="åŒ…å«è¯„è®ºåŒºå†…å®¹")
    parser.add_argument("--max-comments", type=int, default=10, help="æ¯ä¸ªè§†é¢‘çš„æœ€å¤§è¯„è®ºæ•° (é»˜è®¤: 10)")
    parser.add_argument("--json", action="store_true", help="JSON æ ¼å¼è¾“å‡º")
    parser.add_argument("--pretty", action="store_true", help="æ ¼å¼åŒ– JSON è¾“å‡º")
    parser.add_argument("--markdown", action="store_true", help="Markdown æ ¼å¼è¾“å‡º")
    parser.add_argument("-o", "--output", help="ä¿å­˜è¾“å‡ºåˆ°æ–‡ä»¶")
    parser.add_argument("--save-raw", action="store_true", help="ä¿å­˜åŸå§‹å“åº”åˆ° responses/ ç›®å½•")
    parser.add_argument("--env-file", default=".env", help="ç¯å¢ƒå˜é‡æ–‡ä»¶è·¯å¾„")

    return parser.parse_args()


def main():
    args = parse_args()

    # åŠ è½½ç¯å¢ƒå˜é‡
    env_file = Path(__file__).parent.parent.parent / args.env_file
    load_env_file(str(env_file))

    # ç¡®å®š API å¯†é’¥
    api_key = args.api_key or os.getenv("YOUTUBE_API_KEY", "")
    if not api_key:
        print("é”™è¯¯: ç¼ºå°‘ YouTube API å¯†é’¥", file=sys.stderr)
        print("ä½¿ç”¨æ–¹å¼: python youtube_search.py \"å…³é”®è¯\" --api-key YOUR_API_KEY", file=sys.stderr)
        print("æˆ–åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® YOUTUBE_API_KEY", file=sys.stderr)
        return 1

    # ç¡®å®šå…³é”®è¯
    keyword = args.keyword_opt if args.keyword_opt else args.keyword
    if not keyword:
        keyword = os.getenv("YOUTUBE_KEYWORD", "")

    if not keyword:
        print("é”™è¯¯: ç¼ºå°‘æœç´¢å…³é”®è¯", file=sys.stderr)
        print("ä½¿ç”¨æ–¹å¼: python youtube_search.py \"å…³é”®è¯\"", file=sys.stderr)
        return 1

    # æ‰§è¡Œæœç´¢
    try:
        # æœç´¢è§†é¢‘
        video_ids = search_videos(
            api_key=api_key,
            keyword=keyword,
            limit=args.limit,
            order=args.order,
            region_code=args.region,
            language=args.language,
        )

        if not video_ids:
            print(f"æœªæ‰¾åˆ°å…³é”®è¯ '{keyword}' çš„ç›¸å…³è§†é¢‘", file=sys.stderr)
            return 1

        # è·å–è¯¦ç»†ä¿¡æ¯
        results = get_video_details(
            api_key=api_key,
            video_ids=video_ids,
            include_comments=args.include_comments,
            max_comments=args.max_comments,
        )

        if not results:
            print(f"è·å–è§†é¢‘è¯¦ç»†ä¿¡æ¯å¤±è´¥", file=sys.stderr)
            return 1

        # ä¿å­˜åŸå§‹å“åº”
        if args.save_raw:
            responses_dir = Path(__file__).parent / "responses"
            responses_dir.mkdir(exist_ok=True)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            raw_file = responses_dir / f"youtube_search_{timestamp}.json"
            with open(raw_file, "w", encoding="utf-8") as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"åŸå§‹å“åº”å·²ä¿å­˜: {raw_file}", file=sys.stderr)

        # è¾“å‡ºç»“æœ
        output_content = None

        if args.json:
            output_content = json.dumps(results, ensure_ascii=False, indent=2 if args.pretty else None)
        elif args.markdown:
            output_content = format_markdown_output(results, keyword, args.include_comments)
        else:
            format_text_output(results, keyword, args.include_comments)

        # ä¿å­˜åˆ°æ–‡ä»¶
        if args.output and output_content:
            with open(args.output, "w", encoding="utf-8") as f:
                f.write(output_content)
            print(f"\nç»“æœå·²ä¿å­˜åˆ°: {args.output}", file=sys.stderr)
        elif output_content:
            print(output_content)

        return 0

    except Exception as e:
        print(f"æœç´¢å¤±è´¥: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
