#!/usr/bin/env python3
"""åˆ†ææœ€æ–°çš„ union_search æœç´¢ç»“æœå¹¶è¿›è¡Œå»é‡"""

import json
import os
import sys
from pathlib import Path

def get_url(item):
    """ä»ä¸åŒå¹³å°çš„itemä¸­æå–URL"""
    for key in ['url', 'link', 'href', 'html_url']:
        if key in item and item[key]:
            return item[key]
    return None

def normalize_title(title):
    """æ ‡å‡†åŒ–æ ‡é¢˜ç”¨äºæ¯”è¾ƒ"""
    if not title:
        return ""
    # ç§»é™¤ HTML æ ‡ç­¾ (ç®€å•å¤„ç†)
    if isinstance(title, str):
        import re
        title = re.sub(r'<[^>]+>', '', title)
    return title.strip().lower()

def analyze_file(file_path):
    if not os.path.exists(file_path):
        print(f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return

    with open(file_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    # æ”¶é›†æ‰€æœ‰ç»“æœ
    all_items = []
    platforms_data = data.get("results", {})
    for platform, result in platforms_data.items():
        items = result.get("items", [])
        for item in items:
            item_copy = item.copy()
            item_copy["_platform"] = platform
            all_items.append(item_copy)

    print("=" * 60)
    print("ğŸ“Š æœç´¢ç»“æœå»é‡åˆ†ææŠ¥å‘Š")
    print("=" * 60)
    print(f"\nã€åŸºæœ¬ä¿¡æ¯ã€‘")
    print(f"  - å…³é”®è¯: {data.get('keyword')}")
    print(f"  - åŸå§‹æ€»æ•°: {len(all_items)} æ¡")

    # 1. URL å»é‡
    urls_seen = {}
    unique_by_url = []
    duplicates_by_url = []

    for item in all_items:
        url = get_url(item)
        if url:
            # æ ‡å‡†åŒ–URL: ç§»é™¤æœ«å°¾æ–œæ , è½¬å°å†™
            url_norm = url.rstrip('/').lower()
            if url_norm not in urls_seen:
                urls_seen[url_norm] = item
                unique_by_url.append(item)
            else:
                duplicates_by_url.append({
                    "current": item,
                    "original": urls_seen[url_norm]
                })
        else:
            # æ— URLçš„ä¿ç•™
            unique_by_url.append(item)

    # 2. æ ‡é¢˜å»é‡ (åœ¨URLå»é‡åçš„åŸºç¡€ä¸Š)
    titles_seen = {}
    unique_final = []
    duplicates_by_title = []

    for item in unique_by_url:
        title = item.get('title', item.get('name', item.get('full_name', '')))
        norm_title = normalize_title(title)

        if norm_title and norm_title in titles_seen:
            duplicates_by_title.append({
                "current": item,
                "original": titles_seen[norm_title]
            })
        else:
            if norm_title:
                titles_seen[norm_title] = item
            unique_final.append(item)

    print(f"\nã€å»é‡ç»Ÿè®¡ã€‘")
    print(f"  â”œâ”€ åŸå§‹æ€»è®¡:     {len(all_items)} æ¡")
    print(f"  â”œâ”€ URLå»é‡å:    {len(unique_by_url)} æ¡ (å‡å°‘ {len(duplicates_by_url)} æ¡)")
    print(f"  â”œâ”€ æœ€ç»ˆå»é‡å:   {len(unique_final)} æ¡ (å†æ¬¡å‡å°‘ {len(duplicates_by_title)} æ¡)")
    print(f"  â””â”€ æ€»è®¡ç§»é™¤:     {len(all_items) - len(unique_final)} æ¡")

    if duplicates_by_url:
        print(f"\nã€URLé‡å¤é¡¹è¯¦æƒ…ã€‘")
        for dup in duplicates_by_url:
            c = dup['current']
            o = dup['original']
            title = c.get('title', c.get('name', 'N/A'))
            print(f"  âš ï¸  [{c['_platform']}] é‡å¤äº† [{o['_platform']}]: {title[:50]}...")

    if duplicates_by_title:
        print(f"\nã€æ ‡é¢˜ç›¸ä¼¼é‡å¤é¡¹è¯¦æƒ…ã€‘")
        for dup in duplicates_by_title:
            c = dup['current']
            o = dup['original']
            c_title = c.get('title', c.get('name', 'N/A'))
            o_title = o.get('title', o.get('name', 'N/A'))
            print(f"  âš ï¸  [{c['_platform']}] æ ‡é¢˜é‡å¤: {c_title[:40]}")
            print(f"      ä¸ [{o['_platform']}] ç›¸åŒ: {o_title[:40]}")

    print("\n" + "=" * 60)
    print(f"âœ… å¤„ç†å®Œæˆ: æœ€ç»ˆä¿ç•™ {len(unique_final)} æ¡ä¼˜è´¨ç»“æœ")
    print("=" * 60)

    # ä¿å­˜å»é‡åçš„ç»“æœ
    output_path = file_path.replace(".json", "_deduped.json")
    # æ„å»ºæ–°çš„ JSON ç»“æ„
    deduped_data = data.copy()
    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬å¯èƒ½éœ€è¦æŠŠå»é‡åçš„ç»“æœæ”¾å›åŸæ¥çš„ results ç»“æ„ä¸­ï¼Œ
    # æˆ–è€…ç›´æ¥è¾“å‡ºæ‰“æ•£åçš„åˆ—è¡¨ã€‚é€šå¸¸ä¸ºäº†å±•ç¤ºï¼Œæ‰“æ•£åçš„åˆ—è¡¨æ›´æ¸…æ™°ã€‚
    # ä½†ä¸ºäº†ä¿æŒå…¼å®¹ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‰å¹³å°åˆ†ç»„æ”¾å›å»ã€‚
    
    new_results = {}
    for item in unique_final:
        platform = item.pop("_platform")
        if platform not in new_results:
            new_results[platform] = {"items": [], "success": True, "total": 0}
        new_results[platform]["items"].append(item)
        new_results[platform]["total"] = len(new_results[platform]["items"])
    
    deduped_data["results"] = new_results
    deduped_data["summary"]["total_items"] = len(unique_final)
    deduped_data["summary"]["deduped_count"] = len(all_items) - len(unique_final)

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(deduped_data, f, ensure_ascii=False, indent=2)
    print(f"å»é‡ç»“æœå·²ä¿å­˜è‡³: {output_path}")

if __name__ == "__main__":
    target = "test_results_v2.json"
    if len(sys.argv) > 1:
        target = sys.argv[1]
    
    analyze_file(target)
